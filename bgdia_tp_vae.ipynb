{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpuzAeOwbzsl"
      },
      "source": [
        "# Training a VAE on MNIST\n",
        "\n",
        "This notebook was originally written by Stéphane Lathuillière: <br/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://stelat.eu/\n",
        "\n",
        "First, you'll need to enable and test the GPU for the notebook:\n",
        "\n",
        "    Navigate to Edit→Notebook Settings\n",
        "    select GPU from the Hardware Accelerator drop-down\n",
        "    \n",
        "You can check if the GPU works with nvidia-smi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvDUZghtyFtl"
      },
      "source": [
        "## Data loading\n",
        "Let's import some libraries and load the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GmFdb_qht0ES"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "\n",
        "bs = 100\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n06Juz1WyOgS"
      },
      "source": [
        "## Architecture specification\n",
        "\n",
        "You can use the following encoder (don't forget that in a VAE the decoder has two heads, for the mean and the variance):\n",
        "```\n",
        "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
        "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
        "  (fc31): Linear(in_features=256, out_features=2, bias=True)\n",
        "  (fc32): Linear(in_features=256, out_features=2, bias=True)\n",
        "```\n",
        "and this decoder:\n",
        "```\n",
        "  (fc4): Linear(in_features=2, out_features=256, bias=True)\n",
        "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
        "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
        "```\n",
        "Then, you need to complete the forward function. You can use Relu activations for every layer except for the heads that can reach negative values. Concerning the decoder head, you can use sigmoid to obtain values in [0,1].\n",
        "\n",
        "Here we consider a latent space of dimension 2 but is could be any *zdim* dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TXxva1KKxaS0",
        "outputId": "52bacaa3-4af0-4175-f00f-673fa433eeee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-a6a591a01ac0>, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a6a591a01ac0>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    eps = # ...\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
        "\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "\n",
        "\n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h1 = self.fc31(h)\n",
        "        h2 = self.fc32(h)\n",
        "        return h1, h2\n",
        "\n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h))\n",
        "\n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = # ...\n",
        "        return # ... return z sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = # ...\n",
        "        z = # ...\n",
        "        x_rec = # ...\n",
        "        return x_rec, mu, log_var\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_xb6xZzd3L"
      },
      "source": [
        "## Loss function\n",
        "The total loss of the VAE is composed of two terms: a reconstruction term and the KL divergence with the Prior.\n",
        "\n",
        "Assuming a standard prior $\\mathcal{N}(0,Id)$ for $p(z)$ and a Gaussian $\\mathcal{N}(\\mu, \\mathsf{diag}(\\sigma))$ for $q(z|x)$, we recall that\n",
        "$$ KL( q(z|x) \\parallel p(z) ) = \\frac{1}{2} \\left[ - \\sum_i (1 + \\log \\sigma_i^2) + \\sum_i \\sigma_i^2 + \\sum_i \\mu_i^2 \\right] .$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CF SLIDE 25 !!! on fera la loss moyénée suivent tous les xn : sur les données du batch || muthetha décodeur\n",
        "On tire des x et on calcule/ tire des z sachant x (l encodeur nous donne une gaussienne et les z on les obtient en tirant suivant cette gaussienne)\n",
        "\n",
        "Je tire un sample Z et je décode ce sample <-- décodeur"
      ],
      "metadata": {
        "id": "LnKR2ftqOQob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TyaXc-NHxGaZ",
        "outputId": "7676d45c-7af4-4730-9a2a-7b49c3a5e2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-9c98e792113c>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9c98e792113c>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    RecLoss = # ...\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# build model\n",
        "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()\n",
        "optimizer = optim.Adam(vae.parameters())\n",
        "\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(x_rec , x, mu, log_var):\n",
        "    RecLoss = # ...\n",
        "    KLDLoss = # ...\n",
        "    return RecLoss + KLDLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwbBd2mtzqwi"
      },
      "source": [
        "## Training step\n",
        "In this cell, we define the function that performs training on one epoch. We recommend to debug it using the \"main loop\" cell later in the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZysT5cQ7xGre"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ...\n",
        "        loss = # ...\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNYNWVJazxMu"
      },
      "source": [
        "## Visualization:\n",
        "We provide some visualization functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpPtdpf1w8hw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot the loss from each batch\n",
        "def plotLoss(epoch):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(dLosses, label='Discriminitive loss')\n",
        "    plt.plot(gLosses, label='Generative loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('gan_loss_epoch_%d.png' % epoch)\n",
        "\n",
        "# Create a wall of generated MNIST images\n",
        "def plotGeneratedImages(generatedImages,dim=(10, 10), figsize=(10, 10)):\n",
        "    generatedImages=generatedImages.cpu().numpy()\n",
        "    print(generatedImages.shape)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generatedImages.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(np.squeeze(generatedImages[i]), interpolation='nearest', cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
        "    IPython.display.display(IPython.display.Image(data=('gan_generated_image_epoch_%d.png' % epoch)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP7w6d1i0Mrv"
      },
      "source": [
        "## Test\n",
        "Complete the test function that generate new sample from latent space samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJVwV2Ebwt3Y"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            recon, mu, log_var = # ...\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += # ...\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "    with torch.no_grad():\n",
        "      z = torch.randn(64, 2).cuda()\n",
        "      generated = # ...\n",
        "      plotGeneratedImages(generated.view(generated.size(0), 1, 28, 28))\n",
        "      #save_image(sample.view(64, 1, 28, 28), '.sample_' + '.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AkcXsDL0WVH"
      },
      "source": [
        "## Main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8XsOe1Vt-gy"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(1, 5):\n",
        "    train(epoch)\n",
        "test()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URCpsHcs-tnx"
      },
      "source": [
        "## Bonus 1: Interpolation\n",
        "Sample two latent vectors z1 and z2 and generate the corresponding images. Then, generate 10 intermediate images by performing linear interpolation in the latent space between the z1 and z2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtLV1dQEBVNe"
      },
      "outputs": [],
      "source": [
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNvoqzJOAK3C"
      },
      "source": [
        "## Bonus 2: Visualization\n",
        "Generate a 10x10 mosaic by sampling *z* over a regulargrid in [-1,1]x[-1,1]. What do you observe?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDRGnrqNHTAW"
      },
      "outputs": [],
      "source": [
        "# ..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pytorch_VAE_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}